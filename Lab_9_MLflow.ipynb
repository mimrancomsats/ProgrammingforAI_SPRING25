{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimrancomsats/ProgrammingforAI_SPRING25/blob/main/Lab_9_MLflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLFlow**"
      ],
      "metadata": {
        "id": "B1T4jELFEpTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible.**\n",
        "\n",
        "**In this notebook, we are going to use MLFlow for Experiment Tracking**\n",
        "\n",
        "**The installation process of MLFlow is described in the following link:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html"
      ],
      "metadata": {
        "id": "Ip9w5YHZEu7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLFlow Library Installation**"
      ],
      "metadata": {
        "id": "FmZxJ3R3uLUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XEMbJcXdYLAR",
        "outputId": "283d289c-5105-4f01-b07f-95ed7280e51f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (KNN)**"
      ],
      "metadata": {
        "id": "0v6QLYCxh2mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "titanic_data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Custom function to impute missing values in 'Embarked' column\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the 'FamilySize' feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Add 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop columns that are not needed for model training\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create 'FamilySize' and drop 'SibSp' and 'Parch' columns\n",
        "def family_size(X):\n",
        "    #print(X)\n",
        "    X = create_family_size(X)\n",
        "    #print(X)\n",
        "    X = drop_columns(X)\n",
        "    #print(X)\n",
        "    return X\n",
        "\n",
        "# Pipeline to preprocess 'Age' column\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute missing 'Age' values\n",
        "    ('age_scaler', MinMaxScaler())  # Scale 'Age' feature\n",
        "])\n",
        "\n",
        "# Pipeline to preprocess 'Fare' column\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    #('fare_imputer', SimpleImputer(strategy='mean')),  # Optionally impute missing 'Fare'\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale 'Fare' feature\n",
        "])\n",
        "\n",
        "# Pipeline to create and scale the 'FamilySize' feature\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler()),  # Scale 'FamilySize'\n",
        "])\n",
        "\n",
        "# Pipeline to preprocess 'Embarked' column\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute missing 'Embarked' values\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode 'Embarked'\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess all relevant features\n",
        "knn_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('age_encoder', age_pipeline, ['Age']),  # Preprocess 'Age'\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),  # Preprocess 'Fare'\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Preprocess 'FamilySize'\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),  # Preprocess 'Embarked'\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),  # One-hot encode 'Sex'\n",
        "    ('pclass_scaler', MinMaxScaler(), ['Pclass']),  # Scale 'Pclass'\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline with preprocessing and the KNN classifier\n",
        "knn_pipeline = Pipeline(steps=[\n",
        "    ('knn_preprocessor', knn_preprocessor),  # Data preprocessing steps\n",
        "    ('knn_classifier', KNeighborsClassifier(n_neighbors=5))  # KNN Classifier\n",
        "])\n",
        "\n",
        "# Separate features and target variable\n",
        "#X = data.drop('Survived', axis=1)\n",
        "X = titanic_data.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "knn_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nKNN Model Accuracy: {knn_accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix for evaluating the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2FbXPYah0pj",
        "outputId": "65cc5a5c-d4ed-42bf-dbdd-07ea4217cd96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KNN Model Accuracy: 0.80\n",
            "Confusion Matrix:\n",
            "[[90 15]\n",
            " [21 53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "JDTcZM1lupfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name\n",
        "mlflow.set_tracking_uri(uri=\"http://3.85.232.20:5000\")\n",
        "mlflow.set_experiment(\"KNN Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the prameters related to KNN model\n",
        "    mlflow.log_param(\"model\",\"KNN\")\n",
        "    mlflow.log_param(\"n_neighbors\", 5)\n",
        "    mlflow.log_param(\"metric\", 'euclidean')\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", knn_accuracy)\n",
        "\n",
        "    # Log the KNN model (use the knn_pipeline variable)\n",
        "    mlflow.sklearn.log_model(knn_pipeline, \"KNN Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XBnh9LaXb3K",
        "outputId": "98799aee-2042-467d-b30d-dc29d75293a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/05/02 10:37:30 INFO mlflow.tracking.fluent: Experiment with name 'KNN Experiment' does not exist. Creating a new experiment.\n",
            "\u001b[31m2025/05/02 10:37:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run unleashed-pig-897 at: http://3.87.94.8:5000/#/experiments/346416385151607627/runs/c4b68d93f72142fdbf7948de32b9d8d4\n",
            "🧪 View experiment at: http://3.87.94.8:5000/#/experiments/346416385151607627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (Decision Tree)**"
      ],
      "metadata": {
        "id": "HscZRUP0G7xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Custom function to impute missing values in the 'Embarked' column\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the 'FamilySize' feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop specified columns\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create 'FamilySize' and drop 'SibSp' and 'Parch' columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for 'Age'\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute missing 'Age' values\n",
        "    ('age_scaler', MinMaxScaler())  # Scale 'Age' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Fare'\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale 'Fare' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'FamilySize'\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler())  # Scale 'FamilySize' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Embarked'\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute missing 'Embarked' values\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode 'Embarked'\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess the data\n",
        "dt_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop_columns', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),  # Drop irrelevant columns\n",
        "    ('age_encoder', age_pipeline, ['Age']),  # Preprocess 'Age'\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),  # Preprocess 'Fare'\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Preprocess 'FamilySize'\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),  # Preprocess 'Embarked'\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),  # One-hot encode 'Sex'\n",
        "    ('pclass_scaler', MinMaxScaler(), ['Pclass']),  # Scale 'Pclass'\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline that includes preprocessing and the Decision Tree classifier\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    ('dt_preprocessor', dt_preprocessor),  # Data preprocessing steps\n",
        "    ('dt_classifier', DecisionTreeClassifier(criterion='entropy'))  # Decision Tree Classifier\n",
        "])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "dt_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nDecision Tree Model Accuracy: {dt_accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix for evaluating the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcB_MACF8LDc",
        "outputId": "90aad35a-0e12-4ee4-dd51-88ab33e34f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Model Accuracy: 0.77\n",
            "Confusion Matrix:\n",
            "[[82 23]\n",
            " [19 55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "ypLDHQ3Kuwfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name\n",
        "mlflow.set_tracking_uri(uri=\"http://3.87.94.8:5000\")\n",
        "mlflow.set_experiment(\"Decision Tree Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the parameters related to Decision Tree model\n",
        "    mlflow.log_param(\"model\",\"Decision Tree\")\n",
        "    mlflow.log_param(\"criterion\", \"entropy\")\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", dt_accuracy)\n",
        "\n",
        "    # Log the Decision Tree model (use the dt_pipeline variable)\n",
        "    mlflow.sklearn.log_model(dt_pipeline, \"Decision Tree Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvzvmw6X8PP2",
        "outputId": "006e338a-47cc-4c77-a373-3cab183fe407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/05/02 10:39:34 INFO mlflow.tracking.fluent: Experiment with name 'Decision Tree Experiment' does not exist. Creating a new experiment.\n",
            "\u001b[31m2025/05/02 10:39:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run gregarious-goose-418 at: http://3.87.94.8:5000/#/experiments/312805377520945777/runs/e6dc6769f8d847f382c6d97fdd131946\n",
            "🧪 View experiment at: http://3.87.94.8:5000/#/experiments/312805377520945777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sklearn Pipeline Implementation (Random Forest)**"
      ],
      "metadata": {
        "id": "TA2ZdeprHDEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Custom function to impute missing values in the 'Embarked' column\n",
        "def impute_embarked(X):\n",
        "    X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0])  # Fill missing values\n",
        "    return X\n",
        "\n",
        "# Custom function to create the 'FamilySize' feature\n",
        "def create_family_size(X):\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1  # Adding 1 for the individual themselves\n",
        "    return X\n",
        "\n",
        "# Custom function to drop specified columns\n",
        "def drop_columns(X):\n",
        "    return X.drop(['SibSp', 'Parch'], axis=1)\n",
        "\n",
        "# Function to create 'FamilySize' and drop 'SibSp' and 'Parch' columns\n",
        "def family_size(X):\n",
        "    X = create_family_size(X)\n",
        "    X = drop_columns(X)\n",
        "    return X\n",
        "\n",
        "# Create pipelines for 'Age'\n",
        "age_pipeline = Pipeline(steps=[\n",
        "    ('age_imputer', SimpleImputer(strategy='mean')),  # Impute missing 'Age' values\n",
        "    ('age_scaler', MinMaxScaler())  # Scale 'Age' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Fare'\n",
        "fare_pipeline = Pipeline(steps=[\n",
        "    ('fare_scaler', MinMaxScaler())  # Scale 'Fare' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'FamilySize'\n",
        "family_size_pipeline = Pipeline(steps=[\n",
        "    ('family_size_creator', FunctionTransformer(family_size)),\n",
        "    ('family_size_scaler', MinMaxScaler())  # Scale 'FamilySize' feature\n",
        "])\n",
        "\n",
        "# Create pipelines for 'Embarked'\n",
        "embarked_pipeline = Pipeline(steps=[\n",
        "    ('embarked_imputer', FunctionTransformer(impute_embarked)),  # Impute missing 'Embarked' values\n",
        "    ('embarked_onehot', OneHotEncoder())  # One-hot encode 'Embarked'\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to preprocess the data\n",
        "rf_preprocessor = ColumnTransformer(transformers=[\n",
        "    ('drop_columns', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),  # Drop irrelevant columns\n",
        "    ('age_encoder', age_pipeline, ['Age']),  # Preprocess 'Age'\n",
        "    ('fare_encoder', fare_pipeline, ['Fare']),  # Preprocess 'Fare'\n",
        "    ('family_size', family_size_pipeline, ['SibSp', 'Parch']),  # Preprocess 'FamilySize'\n",
        "    ('embarked_encoder', embarked_pipeline, ['Embarked']),  # Preprocess 'Embarked'\n",
        "    ('sex_encoder', OneHotEncoder(), ['Sex']),  # One-hot encode 'Sex'\n",
        "    ('pclass_scaler', MinMaxScaler(), ['Pclass']),  # Scale 'Pclass'\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Create a complete pipeline that includes preprocessing and the Random Forest classifier\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('rf_preprocessor', rf_preprocessor),  # Data preprocessing steps\n",
        "    ('rf_classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest Classifier\n",
        "])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Survived', axis=1)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nRandom Forest Model Accuracy: {rf_accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix for evaluating the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAy0Iw6Y-SoE",
        "outputId": "c0ebc743-a9f1-4c5a-f7ed-5a612cc680fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Model Accuracy: 0.82\n",
            "Confusion Matrix:\n",
            "[[91 14]\n",
            " [19 55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Tracking**"
      ],
      "metadata": {
        "id": "X2B-ns2ou0Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Set the tracking URI and experiment name for Random Forest\n",
        "mlflow.set_tracking_uri(uri=\"http://3.87.94.8:5000\")\n",
        "mlflow.set_experiment(\"Random Forest Experiment\")\n",
        "\n",
        "# Start a new MLflow run\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Log the hyperparameters\n",
        "    mlflow.log_param(\"model\",\"Random Forest\")\n",
        "    mlflow.log_param(\"n_estimators\", 100)\n",
        "    mlflow.log_param(\"random_state\", 42)\n",
        "\n",
        "    # Log the accuracy metric\n",
        "    mlflow.log_metric(\"accuracy\", rf_accuracy)\n",
        "\n",
        "    # Log the Random Forest model (use the rf_pipeline variable)\n",
        "    mlflow.sklearn.log_model(rf_pipeline, \"Random Forest Algorithm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeeduLjA-fdd",
        "outputId": "c6e70365-6f10-4b59-9126-fb4252216248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/05/02 10:41:09 INFO mlflow.tracking.fluent: Experiment with name 'Random Forest Experiment' does not exist. Creating a new experiment.\n",
            "\u001b[31m2025/05/02 10:41:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run redolent-shoat-348 at: http://3.87.94.8:5000/#/experiments/707553264580851716/runs/2009dc12adbe4c049cb25872ed038a81\n",
            "🧪 View experiment at: http://3.87.94.8:5000/#/experiments/707553264580851716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Task\n",
        "\n",
        "1. Extend the experiment tracking code to log precision, recall and F1 score metrics in the Mlflow tool.\n",
        "2. Register the best performing model in the Mlflow tool.\n",
        "3. Perform Inference over test set using the model registered in the Step 2.\n",
        "4. Perform the steps mentioned above on the following dataset.\n",
        "\n",
        "https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ],
      "metadata": {
        "id": "Vu7nZz_DrFFY"
      }
    }
  ]
}